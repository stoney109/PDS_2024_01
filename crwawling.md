# 📦 크롤링 작업 문서

이 문서는 각 크롤링 스크립트의 역할과 구현 세부 사항을 설명합니다. 각 파일은 특정 데이터를 수집하고 저장하기 위한 Python 코드로 작성되었습니다.

---

## 📜 목차
1. [네이버 카페 데이터 크롤링 (`cafe_crwaling.py`)](#네이버-카페-데이터-크롤링)
2. [농림 데이터 API 크롤링 (`nonglim_api_crwaling.py`)](#농림-데이터-api-크롤링)
3. [서울 데이터 API 크롤링 (`seoul_api_crawling.py`)](#서울-데이터-api-크롤링)

---

## 네이버 카페 데이터 크롤링

### **파일**: `cafe_crwaling.py`

- **설명**: 네이버 카페의 게시글을 크롤링하여 게시글의 제목, 텍스트, 썸네일, 작성일 등의 데이터를 수집합니다.
- **주요 작업**:
  1. `selenium`을 사용해 네이버 카페 페이지의 iframe에 접근.
  2. 게시글 목록과 상세 데이터를 크롤링.
  3. `BeautifulSoup`으로 HTML 구조 파싱 및 데이터 추출.
  4. 수집한 데이터를 `DataFrame`으로 저장하고 CSV 파일로 출력.

- **사용된 주요 라이브러리**:
  - `selenium`: 웹페이지 동적 크롤링.
  - `BeautifulSoup`: HTML 파싱 및 데이터 추출.
  - `pandas`: 수집 데이터의 테이블화 및 CSV 저장.

- **출력 파일**:
  - `cafe_crawling_2024.csv`: 크롤링된 게시글 데이터.

---

## 농림 데이터 API 크롤링

### **파일**: `nonglim_api_crwaling.py`

- **설명**: 농림 데이터 API에서 유기 동물 데이터를 요청하여 각 동물의 정보(품종, 색상, 나이 등)를 수집합니다.
- **주요 작업**:
  1. API 요청을 통해 XML 데이터를 수신.
  2. `BeautifulSoup`을 사용해 XML 데이터를 파싱.
  3. 각 동물의 상세 정보를 리스트에 저장.
  4. 데이터프레임으로 변환 후 CSV 파일로 저장.

- **사용된 주요 라이브러리**:
  - `requests`: API 요청.
  - `BeautifulSoup`: XML 데이터 파싱.
  - `pandas`: 수집 데이터의 테이블화 및 CSV 저장.

- **출력 파일**:
  - `nonglim_crawling_2024.csv`: API로 수집된 농림 데이터.

---

## 서울 데이터 API 크롤링

### **파일**: `seoul_api_crawling.py`

- **설명**: 서울동물복지지원센터 API에서 보호 동물 데이터를 요청하여 정보(견종, 보호 장소 등)를 수집합니다.
- **주요 작업**:
  1. API 요청을 통해 JSON 데이터를 수신.
  2. JSON 데이터를 파싱하여 주요 정보를 추출.
  3. 컬럼 이름을 통일하고 데이터프레임으로 저장.
  4. CSV 파일로 출력.

- **사용된 주요 라이브러리**:
  - `requests`: API 요청 및 데이터 수신.
  - `pandas`: 수집 데이터의 테이블화 및 CSV 저장.

- **출력 파일**:
  - `seoul_crawling_2024.csv`: API로 수집된 서울 데이터.

---

## ✨ 문서 목적

이 문서는 크롤링 작업의 재현 가능성과 유지보수를 위해 작성되었습니다. 각 스크립트의 역할 및 사용법을 명확히 설명하여 데이터 수집 및 전처리 작업에 기여합니다.